{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Description : Working Nb for code either currently in development, not fully documented, or simply not yet part of a self-contained pipeline.\n",
    "\n",
    "# Date    : 2024-06-11 10:37:25\n",
    "# Author  : Karla Onate Melecio <kgom.astro@gmail.com>\n",
    "# Version : 3.10.12\n",
    "\n",
    "\n",
    "import sys\n",
    "import struct\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function: pull_feather_files_from_L0_directory(run_id, channels, ps_window=30000)**\n",
    "\n",
    "\n",
    "This function processes all files in the given directory and returns the tagged, singles, and \"all\" data for the specified list of channels. This assumes you have already produced the event lookup table using the data level processing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_feather_files_from_L0_directory(run_id, channels, ps_window=30000):\n",
    "    \"\"\"\n",
    "    Process all files in the given directory and returns the tagged, singles, and \"all\" data for the specified list of channels.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    run_ids : list of int\n",
    "        List of run IDs to process.\n",
    "    channels : list of int\n",
    "        List of channel IDs to include in the analysis.\n",
    "    ps_window : int, optional\n",
    "        The peak search window size (default is 30000).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    all_dfs : list of pd.DataFrame\n",
    "        DataFrames containing processed data for each run.\n",
    "    tagged : list of pd.DataFrame\n",
    "        DataFrames containing tagged events.\n",
    "    singles : list of pd.DataFrame\n",
    "        DataFrames containing singles events.\n",
    "\n",
    "    TODO:\n",
    "    -------\n",
    "    Implement multi-event functionality; singles can likely be removed as the dataframe with \"all\" data contain a column with 'multiplicity' information.\n",
    "    \"\"\"\n",
    "    singles =[]\n",
    "    tagged = []\n",
    "    all_dfs = []\n",
    "\n",
    "    for run_id in run_id:\n",
    "        # Read L0 data\n",
    "        data = pd.read_feather(f'../data/L0/L0_{run_id}.feather')\n",
    "        data['hit_id'] = data.index.values\n",
    "        data['sec'] = ((data.time - data.time.min()) // 1e12).astype('int64')\n",
    "\n",
    "\n",
    "        # Calculate rate for each second\n",
    "        seconds, rate = np.unique(data.sort_values(by='sec').sec, return_counts=True)\n",
    "        rate_data = pd.DataFrame({'sec': seconds, 'rate': rate})\n",
    "        data = pd.merge(data, rate_data, on='sec')\n",
    "\n",
    "        # Read event lookup table\n",
    "        event_table = pd.read_feather(f'../data/ancillary/lookup_tables/event/events_{ps_window}ps_window_{run_id}.feather')\n",
    "        event_table = event_table.rename(columns={'hits': 'hit_id'})\n",
    "        \n",
    "\n",
    "        # Filter data for tagged detectors and specific channels\n",
    "        tdata = data[data.channelID.isin([81, 83])]\n",
    "        gdata = data[data.channelID.isin(channels)]\n",
    "\n",
    "        # Merge with event table for tagged events\n",
    "        cal_events = event_table[event_table.hit_id.isin(tdata.hit_id.values)]\n",
    "        t_events = pd.merge(tdata, cal_events, on='hit_id')\n",
    "\n",
    "\n",
    "        # Merge with event table for gagg events\n",
    "        g_events = event_table[event_table.hit_id.isin(gdata.hit_id.values)]\n",
    "        gdata = pd.merge(gdata, g_events, on='hit_id')\n",
    "\n",
    "\n",
    "        # Merge 2-site tagged events\n",
    "        tgd = pd.merge(gdata, t_events, on='event_id', suffixes=('', '_t'))\n",
    "        tgd = tgd[['event_id', 'sec', 'multiplicity', 'channelID', 'channelID_t', 'adc', 'adc_t', 'rate', 'time', 'time_t']]\n",
    "\n",
    "        # Calculate time difference between hits\n",
    "        tgd['dt'] = tgd.time - tgd.time_t\n",
    "\n",
    "        # Append results to lists\n",
    "        all_dfs.append(gdata)\n",
    "        tagged.append(tgd)\n",
    "        singles.append(gdata[gdata.multiplicity==1])\n",
    "        \n",
    "    \n",
    "\n",
    "    return all_dfs, tagged, singles "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
