{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Description : Data processing codes GRAPE instrument data \n",
    "\n",
    "# Date    : 2024-05-28 16:45:06\n",
    "# Author  : Karla Onate Melecio <kgom.astro@gmail.com>\n",
    "# Version : 3.10.12\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import struct\n",
    "import pathlib\n",
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "\n",
    "# os.environ['NUMEXPR_MAX_THREADS'] = '12'\n",
    "# os.environ['NUMEXPR_NUM_THREADS'] = '8'\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "#                     handlers=[\n",
    "#                         logging.FileHandler(\"nb_log.txt\"),\n",
    "#                         logging.StreamHandler()\n",
    "#                     ])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "\n",
    "sys.path.append(\"functions/\")\n",
    "import data_level_functions\n",
    "from data_level_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust as needed (okay to skip raw data processing if input is .root)\n",
    "raw_file_directory = '../data/raw/'  # expects .root or .ldat files to be in directory\n",
    "raw_file_format = '.root'\n",
    "output_file_format = '.feather' # can specify .pkl.gzip, .csv, or .feather - .root output not currently supported; but can be implented\n",
    "\n",
    "\n",
    "L0_file_directory = '../data/L0/'\n",
    "\n",
    "# this step produces the look up tables for coincident events\n",
    "L2_output_directory = \"../data/L2/\"\n",
    "L2_lookup_directory = \"../data/ancillary/\"\n",
    "coincidence_window = 30000 # this is the time window for coincident events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This step puts the raw file in a readable format and is primarily intended for .ldat files, but .root is supported. The returned file only contained three columns: 'time', 'channelID', 'energy'. You can skip this step if your input file is a .root file**\n",
    "\n",
    "\n",
    "The raw data is first pre-processed with the convert_raw_data_format function which process the raw .ldat file from the Petsys system into a pandas DataFrame containing the unprocessed raw data. convert_raw_data_format utilizes a conversion function called convert_binary_to_dictionary. The convert_binary_to_dictionary function processes a binary file containing petsys \"singles\" data (all triggers) by reading it in chunks of 16 bytes, each representing a timestamp (ps time tag relative to system start time), energy value (digitized measure of charge), and 'channelID' (detector identification number). It then unpacks these values and stores them in lists. Once all data is read, it constructs a dictionary with keys 'time', 'channelID', and 'energy', where each key corresponds to the respective list of values. The convert_raw_data_format function, offers the flexibility to specify the output format, including Feather, compressed CSV, or compressed Pickle files. This step converts binary data into a structured format and saves it in various file formats specified by the user for further analysis or storage.\n",
    "\n",
    "The process_raw_data_directory function iterates through the .ldat files in a specified directory. For each file, it extracts the run ID, invokes the convert_raw_data function to process the binary file into a structured DataFrame, and saves it in the desired output format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_raw_data_directory(directory=raw_file_directory , output_format=output_file_format, input_format=raw_file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions form a pipeline for preprocessing raw data to the L0 data level.\n",
    "\n",
    "1. **convert_raw_adc_to_adc(raw_adc_df)**: This function takes a DataFrame containing raw ADC data and converts it to ADC values using a specific formula. It drops the 'energy' column and returns a DataFrame with 'adc' values.\n",
    "\n",
    "2. **remove_adc_glitches(adc_df, new_glitch_min=None, new_glitch_max=None)**: This function removes glitches from ADC data. It filters the ADC values based on predefined conditions and optionally allows specifying new glitch intervals. It returns a DataFrame with glitches removed.\n",
    "\n",
    "3. **L0_data_preprocess(input_data, input_data_format, output_format, run_id=None)**: This is the main function that orchestrates the preprocessing pipeline. It takes input data in various formats (pickle, CSV, feather, or DataFrame), converts it to a DataFrame, applies ADC conversion, removes glitches, and optionally saves the processed data in L0 format. If run_id is provided, it saves the processed data; otherwise, it returns the cleaned DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convert_raw_adc_to_adc function accepts a DataFrame containing raw ADC data and computes the corresponding ASIC-corrected ADC values based on predefined coefficients. It applies a mathematical formula to each energy value in the DataFrame, resulting in a new column named 'adc' representing the corrected ADC values. The remove_adc_glitches function then filters out known glitches from the ADC data, removing values falling within specified intervals. Both functions provide information on the number of triggers before and after filtering. Finally, the L0_data_preprocess function preprocesses raw data to the L0 data level, converting ADC channels to ADC values if necessary and removing glitches. It saves the processed data in the specified output format, appending the run ID to the output file name if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_L0_data_from_raw(raw_directory=raw_file_directory, L0_directory=L0_file_directory, input_format=raw_file_format, output_format=output_file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 data is currently under revision, this is data with ancillary data included, but this step is currently being done in the processing using lookup tables. Refer to section labeled \"Under Revision\" at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_L0_file function extracts L0 data file and returns the processed dataframes along with the run ID. The identify_events function processes events from the time-sorted data based on a specified time window, identifies singles events, and generates an event lookup DataFrame. The preprocess_L2_data function preprocesses L0 data files to extract singles events, saves the processed singles data, and event lookup information. It iterates over all L0 data files in a directory, processes each file, and saves the results to specified output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2\n",
    "\n",
    "preprocess_L2_data(L0_file_directory, L2_output_directory, L2_lookup_directory, window=coincidence_window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_L1_data_from_L0(L0_directory, L1_directory):\n",
    "#     \"\"\"\n",
    "#         Processes L0 data files from the L0 directory, adds ancillary information (columnID, scintillatorType, PosZ, PosX, PosY),\n",
    "#         and saves the processed data into the L1 directory in both Feather and Pickle formats.\n",
    "\n",
    "#         Parameters:\n",
    "#         L0_directory (str): Path to the directory containing L0 data files.\n",
    "#         L1_directory (str): Path to the directory where L1 data files will be saved.\n",
    "\n",
    "#         Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     # gets new datafiles with picosecond resolution\n",
    "#     data = pathlib.Path(L0_directory)\n",
    "\n",
    "#     # creates a list of the paths to the binary files in the data folder\n",
    "#     files = list(data.glob(\"*.feather\"))\n",
    "\n",
    "#     for i in files:\n",
    "#         s = str.split(str(i), sep='L0\\\\')[1]\n",
    "#         rid = re.search('(.*).feather', s).group(1)\n",
    "#         rid = re.search('L0_(.*)', rid).group(1)\n",
    "\n",
    "#         print(rid)\n",
    "#         data_0 = pd.read_feather(i)\n",
    "#         data_1 = add_ancillaries_to_data(data_0, identifying_data='../data/ancillary/identifying_data.csv', drop_cols=['biasID', 'pinID'])\n",
    "#         logging.info('L1 data created')\n",
    "\n",
    "#         data_1.to_feather(L1_directory + 'L1_' +rid+'.feather')\n",
    "#         logging.info('L1 data feathered')\n",
    "\n",
    "#         data_1.to_pickle(L1_directory + 'L1_' +rid + '.pkl.gzip', compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\n",
    "#         logging.info('L1 data pickled')\n",
    "#     return\n",
    "\n",
    "# process_L1_data_from_L0(\"../data/L0/\", \"../data/L1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_l2a1_file(file_path, adc_summary=None, nbins=601, min_adc=0, max_adc=300):\n",
    "#     \"\"\"\n",
    "#     Process an L2A1 file to produce a DataFrame containing binned ADC spectra for each channelID.\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     file_path : str or Path\n",
    "#         Path to the L2A1 feather file.\n",
    "#     nbins : int\n",
    "#         Number of bins for the ADC histogram.\n",
    "#     min_adc : int\n",
    "#         Minimum ADC value for histogram range.\n",
    "#     max_adc : int\n",
    "#         Maximum ADC value for histogram range.\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     DataFrame\n",
    "#         A DataFrame containing binned ADC spectra for each channelID.\n",
    "#     \"\"\"\n",
    "#     # Read the feather file\n",
    "#     data = pd.read_feather(file_path)\n",
    "    \n",
    "#     # Filter out calibration source detectors\n",
    "#     data = data[(data.channelID != 81) & (data.channelID != 83)]\n",
    "#     data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     # Extract run ID from the filename\n",
    "#     filename = file_path.stem\n",
    "#     run_id = re.search('L2A1_(.*)', filename).group(1)\n",
    "    \n",
    "#     # Initialize an empty DataFrame to store results\n",
    "#     results = pd.DataFrame()\n",
    "\n",
    "#     # Get unique channel IDs\n",
    "#     channel_ids = np.unique(data.channelID)\n",
    "    \n",
    "#     # Process each channel ID\n",
    "#     for channel_id in channel_ids:\n",
    "#         # Filter data for the current channel ID\n",
    "#         channel_data = data[data['channelID'] == channel_id]['adc']\n",
    "\n",
    "#         # if len(adc_summary)>0:\n",
    "#         #     min_adc = adc_summary[adc_summary['detID']==channel_id].min_adc.values\n",
    "#         #     max_adc = adc_summary[adc_summary['detID']==channel_id].max_adc.values\n",
    "#         #     nbins = (max_adc - min_adc)+1\n",
    "#         #     print(channel_id, nbins)\n",
    "        \n",
    "#         # Compute histogram\n",
    "#         hist, bin_edges = np.histogram(channel_data.values.round(1), bins=nbins, range=[min_adc, max_adc])\n",
    "        \n",
    "#         # Create a DataFrame for this channel\n",
    "#         df = pd.DataFrame({\n",
    "#             'channelID': channel_id,\n",
    "#             'bin_edges': bin_edges[:-1].round(1),  # Use left edges of bins\n",
    "#             'counts': hist\n",
    "#         })\n",
    "        \n",
    "#         # Append to the results DataFrame\n",
    "#         results = pd.concat([results, df], ignore_index=True)\n",
    "    \n",
    "#     return results, run_id\n",
    "\n",
    "# def process_l2_directory(directory, adc_summary=None, nbins=601, min_adc=0, max_adc=300):\n",
    "#     \"\"\"\n",
    "#     Process all L2A1 files in the given directory to produce a list of DataFrames containing binned ADC spectra for each channelID.\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     directory : str or Path\n",
    "#         Path to the directory containing the L2A1 files.\n",
    "#     nbins : int\n",
    "#         Number of bins for the ADC histogram.\n",
    "#     min_adc : int\n",
    "#         Minimum ADC value for histogram range.\n",
    "#     max_adc : int\n",
    "#         Maximum ADC value for histogram range.\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     tuple\n",
    "#         A tuple containing a list of DataFrames and a list of run IDs.\n",
    "#     \"\"\"\n",
    "#     data_dir = pathlib.Path(directory)\n",
    "#     files = list(data_dir.glob(\"*.feather\"))\n",
    "\n",
    "#     all_results = []\n",
    "#     run_id_list = []\n",
    "\n",
    "#     for file_path in tqdm(files, desc=\"Processing files\"):\n",
    "#         file_results, run_id = process_l2a1_file(file_path, adc_summary, nbins, min_adc, max_adc)\n",
    "#         all_results.append(file_results)\n",
    "#         run_id_list.append(run_id)\n",
    "\n",
    "#     return all_results, run_id_list\n",
    "\n",
    "# # Example usage\n",
    "# directory = \"../data/L2/\"\n",
    "# adc_spectra_list, run_id_list = process_l2_directory(directory, nbins=2002, min_adc=0, max_adc=1000)\n",
    "\n",
    "\n",
    "# # Print the first DataFrame and corresponding run_id as an example\n",
    "# print(\"First DataFrame:\")\n",
    "# print(adc_spectra_list[0])\n",
    "# print(\"Corresponding run_id:\", run_id_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
